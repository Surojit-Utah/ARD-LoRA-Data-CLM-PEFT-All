{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b87dd1f",
   "metadata": {},
   "source": [
    "# ARD-LoRA Training with Uncertainty Evaluation on Google Colab\n",
    "\n",
    "This notebook trains ARD-LoRA (Automatic Relevance Determination LoRA) models with comprehensive uncertainty evaluation using:\n",
    "\n",
    "üéØ **Key Features:**\n",
    "- **Bayesian-PEFT Dataset Integration**: Compatible with https://github.com/Wang-ML-Lab/bayesian-peft\n",
    "- **ARD-LoRA for LLaMA2-7B**: Injects ProbLoRA into q/k/v/o projections\n",
    "- **Uncertainty Evaluation**: ACC, ECE, NLL metrics after each epoch\n",
    "- **Google Drive Caching**: Persistent dataset storage across sessions\n",
    "- **Complete Callback System**: Prior estimation, latent plotting, evaluation\n",
    "\n",
    "üìä **Training Pipeline:**\n",
    "1. Mount Google Drive for persistent caching\n",
    "2. Install dependencies and setup environment\n",
    "3. Load LLaMA2-7B with ARD-LoRA injection\n",
    "4. Load Bayesian-PEFT datasets with caching\n",
    "5. Train with uncertainty evaluation after each epoch\n",
    "6. Save results and model checkpoints\n",
    "\n",
    "‚ö° **Expected Runtime**: ~2-3 hours for 3 epochs with T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901676e9",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive & Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Verify mount\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "    print(f\"üìÅ Available space: {os.statvfs('/content/drive/MyDrive').f_bavail * os.statvfs('/content/drive/MyDrive').f_frsize // (1024**3)} GB\")\n",
    "else:\n",
    "    print(\"‚ùå Google Drive mount failed!\")\n",
    "\n",
    "# Create ARD-LoRA directories\n",
    "ard_lora_dir = \"/content/drive/MyDrive/ARD_LoRA_Training\"\n",
    "cache_dir = f\"{ard_lora_dir}/DataCache\"\n",
    "results_dir = f\"{ard_lora_dir}/Results\"\n",
    "\n",
    "os.makedirs(ard_lora_dir, exist_ok=True)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ ARD-LoRA directory: {ard_lora_dir}\")\n",
    "print(f\"üíæ Cache directory: {cache_dir}\")\n",
    "print(f\"üìä Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d317d6",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f504786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages not available in default Colab\n",
    "!pip install -q transformers>=4.21.0\n",
    "!pip install -q torch>=2.0.0\n",
    "!pip install -q datasets>=2.5.0\n",
    "!pip install -q accelerate>=0.20.0\n",
    "!pip install -q peft>=0.4.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q tensorboard\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988fe3d",
   "metadata": {},
   "source": [
    "## Step 3: Clone ARD-LoRA Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone the ARD-LoRA repository\n",
    "repo_dir = \"/content/ARD-LoRA-Data-CLM\"\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/Surojit-Utah/ARD-LoRA-Data.git /content/ARD-LoRA-Data-temp\n",
    "    # Copy the CLM-specific implementation\n",
    "    !cp -r /content/ARD-LoRA-Data-temp/ARD-LoRA-Data-CLM /content/\n",
    "    !rm -rf /content/ARD-LoRA-Data-temp\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists!\")\n",
    "\n",
    "# Add to Python path\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(repo_dir)\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify key files exist\n",
    "key_files = [\n",
    "    \"run_training_cached.py\",\n",
    "    \"model/model_llama.py\", \n",
    "    \"trainer/trainer_clm.py\",\n",
    "    \"dataloader/bayesian_peft_cached.py\",\n",
    "    \"evaluate/uncertainty_metrics.py\"\n",
    "]\n",
    "\n",
    "for file in key_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} - MISSING!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819731cc",
   "metadata": {},
   "source": [
    "## Step 4: Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters for Colab\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Colab-optimized configuration\n",
    "colab_config = {\n",
    "    \"defaults\": {\n",
    "        \"runId\": 1,\n",
    "        # Cache configuration for Google Drive\n",
    "        \"cache_root\": cache_dir,\n",
    "        \"google_drive_cache\": cache_dir,\n",
    "        \"use_google_drive\": True,\n",
    "        \n",
    "        # Model configuration optimized for Colab T4\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"lr_scheduler_type\": \"linear\",\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"train_epochs\": 3,\n",
    "        \"rank\": 32,  # Reduced for Colab memory\n",
    "        \"max_len\": 1024,  # Reduced for Colab memory\n",
    "        \"batch_size\": 2,  # Small batch size for T4\n",
    "        \"gradient_accumulation_steps\": 16,  # Compensate for small batch\n",
    "        \"fp16\": True,\n",
    "        \"load_in_4bit\": True,  # Essential for Colab memory\n",
    "        \"prior_var\": 1.0,\n",
    "        \"kl_loss_beta\": 0.01,\n",
    "        \n",
    "        # ARD and uncertainty configuration\n",
    "        \"ard_prior_samples\": 500,  # Reduced for speed\n",
    "        \"ard_prior_ratio\": 0.5,\n",
    "        \"uncertainty_eval_samples\": 500,  # Reduced for speed\n",
    "        \"uncertainty_n_bins\": 15,\n",
    "        \n",
    "        # Callback configuration\n",
    "        \"enable_callbacks\": True,\n",
    "        \"enable_plotting\": True,\n",
    "        \"enable_resampling\": False,  # Disabled for simplicity\n",
    "        \"plot_start_epoch\": 2,\n",
    "        \"plot_interval\": 1,\n",
    "        \n",
    "        # Logging\n",
    "        \"report_to\": [\"tensorboard\"],\n",
    "        \"logging_steps\": 10,\n",
    "        \"eval_steps\": None,\n",
    "        \"save_steps\": None,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\"\n",
    "    },\n",
    "    \n",
    "    \"models\": {\n",
    "        \"LLaMA2-7B\": {\n",
    "            \"model_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
    "            \"tokenizer_name\": \"meta-llama/Llama-2-7b-hf\",\n",
    "            \"train_type\": \"causal_lm\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"defaults\": {\n",
    "                \"rank\": 32,\n",
    "                \"max_len\": 1024,\n",
    "                \"batch_size\": 2,\n",
    "                \"kl_loss_beta\": 0.005\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"datasets\": {\n",
    "        \"BayesianPEFT\": {\n",
    "            \"dataset_type\": \"bayesian_peft\",\n",
    "            \"dataset_name\": \"alpaca\",  # Start with Alpaca\n",
    "            \"dataset_class\": \"S2SDataset\",\n",
    "            \"repo_url\": \"https://github.com/Wang-ML-Lab/bayesian-peft\",\n",
    "            \"cache_root\": cache_dir,\n",
    "            \"max_len\": 1024,\n",
    "            \"num_labels\": 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "os.makedirs(\"config\", exist_ok=True)\n",
    "with open(\"config/colab_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(colab_config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Colab configuration saved!\")\n",
    "print(f\"üìä Training Configuration:\")\n",
    "print(f\"   - Model: LLaMA2-7B (4-bit quantized)\")\n",
    "print(f\"   - Dataset: Alpaca (Bayesian-PEFT compatible)\")\n",
    "print(f\"   - Epochs: {colab_config['defaults']['train_epochs']}\")\n",
    "print(f\"   - Rank: {colab_config['defaults']['rank']}\")\n",
    "print(f\"   - Batch Size: {colab_config['defaults']['batch_size']}\")\n",
    "print(f\"   - Max Length: {colab_config['defaults']['max_len']}\")\n",
    "print(f\"   - Cache: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa59216",
   "metadata": {},
   "source": [
    "## Step 5: Update Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the config loading to use our Colab configuration\n",
    "import yaml\n",
    "\n",
    "# Load our Colab configuration\n",
    "with open(\"config/colab_config.yaml\", \"r\") as f:\n",
    "    COLAB_CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# Override the CONFIG in config/__init__.py\n",
    "import config\n",
    "config.CONFIG = COLAB_CONFIG\n",
    "\n",
    "print(\"‚úÖ Configuration updated for Colab!\")\n",
    "print(f\"üîß Using custom Colab configuration with Google Drive caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35050d4c",
   "metadata": {},
   "source": [
    "## Step 6: Import ARD-LoRA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51404970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all ARD-LoRA components\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import ARD-LoRA components\n",
    "from model.model_llama import inject_problora_llama, ProbLoRALayer\n",
    "from trainer.trainer_clm import ARDCLMTrainer, build_clm_trainer, create_ard_callbacks\n",
    "from dataloader.bayesian_peft_cached import load_bayesian_peft_with_caching\n",
    "from evaluate.uncertainty_metrics import UncertaintyEvaluator\n",
    "from utils.io import get_output_dirs, free_memory\n",
    "\n",
    "# Import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All ARD-LoRA components imported successfully!\")\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"üöÄ GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available - training will be very slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539b45c",
   "metadata": {},
   "source": [
    "## Step 7: Load LLaMA2-7B Model with ARD-LoRA Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6273df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA2-7B with 4-bit quantization and inject ARD-LoRA\n",
    "def load_ard_lora_model():\n",
    "    print(\"üîÑ Loading LLaMA2-7B model...\")\n",
    "    \n",
    "    # 4-bit quantization for Colab memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Inject ARD-LoRA into all attention projections\n",
    "    print(\"üîÑ Injecting ARD-LoRA layers...\")\n",
    "    model = inject_problora_llama(\n",
    "        model,\n",
    "        rank=32,           # Colab-optimized rank\n",
    "        scaling=1.0,\n",
    "        prior_var=1.0,\n",
    "        num_tokens=1024,   # Colab-optimized sequence length\n",
    "        ard_prior_samples=500\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"‚úÖ ARD-LoRA injection completed!\")\n",
    "    print(f\"üìä Model Statistics:\")\n",
    "    print(f\"   - Total parameters: {total_params:,}\")\n",
    "    print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   - Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # Count ARD-LoRA layers\n",
    "    ard_layers = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, ProbLoRALayer):\n",
    "            ard_layers += 1\n",
    "    print(f\"   - ARD-LoRA layers: {ard_layers}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load_ard_lora_model()\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"üßπ Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce819c89",
   "metadata": {},
   "source": [
    "## Step 8: Load Bayesian-PEFT Dataset with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45429b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bayesian-PEFT compatible dataset with Google Drive caching\n",
    "def load_dataset_with_caching():\n",
    "    print(\"üîÑ Loading Bayesian-PEFT dataset with caching...\")\n",
    "    \n",
    "    # Configuration for dataset loading\n",
    "    dataset_config = {\n",
    "        \"dataset_name\": \"alpaca\",\n",
    "        \"max_len\": 1024,\n",
    "        \"cache_root\": cache_dir,\n",
    "        \"batch_size\": 2,\n",
    "        \"num_labels\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load with Bayesian-PEFT compatibility\n",
    "        train_ds, val_ds, tokenizer_updated = load_bayesian_peft_with_caching(\n",
    "            dataset_name=\"alpaca\",\n",
    "            tokenizer_name=\"meta-llama/Llama-2-7b-hf\",\n",
    "            config=dataset_config,\n",
    "            cache_root=cache_dir\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Dataset loaded successfully!\")\n",
    "        print(f\"üìä Dataset Statistics:\")\n",
    "        print(f\"   - Training samples: {len(train_ds) if train_ds else 0}\")\n",
    "        print(f\"   - Validation samples: {len(val_ds) if val_ds else 0}\")\n",
    "        print(f\"   - Cache location: {cache_dir}\")\n",
    "        \n",
    "        # Check cache size\n",
    "        cache_size = sum(f.stat().st_size for f in Path(cache_dir).glob('**/*') if f.is_file()) / (1024**2)\n",
    "        print(f\"   - Cache size: {cache_size:.1f} MB\")\n",
    "        \n",
    "        return train_ds, val_ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Dataset loading failed: {e}\")\n",
    "        print(\"üîÑ This is expected on first run - datasets will be downloaded and cached\")\n",
    "        raise\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_dataset, val_dataset = load_dataset_with_caching()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Dataset loading issue: {e}\")\n",
    "    print(\"Please check the error and re-run if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5af0cf",
   "metadata": {},
   "source": [
    "## Step 9: Create ARD-LoRA Trainer with Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced ARD-LoRA trainer with uncertainty evaluation\n",
    "def create_ard_trainer(model, tokenizer, train_ds, val_ds):\n",
    "    print(\"üîÑ Creating ARD-LoRA trainer...\")\n",
    "    \n",
    "    # Setup output directories\n",
    "    output_dir = f\"{results_dir}/ARD_LoRA_LLaMA2-7B_Alpaca_Colab\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training configuration optimized for Colab\n",
    "    training_config = {\n",
    "        \"train_epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"lr_scheduler_type\": \"linear\",\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"fp16\": True,\n",
    "        \"beta\": 0.01,  # KL regularization strength\n",
    "        \n",
    "        # ARD configuration\n",
    "        \"ard_prior_ratio\": 0.5,\n",
    "        \"ard_prior_samples\": 500,\n",
    "        \"uncertainty_eval_samples\": 500,\n",
    "        \"uncertainty_n_bins\": 15,\n",
    "        \n",
    "        # Callback configuration\n",
    "        \"enable_callbacks\": True,\n",
    "        \"enable_plotting\": True,\n",
    "        \"enable_resampling\": False,\n",
    "        \"plot_start_epoch\": 2,\n",
    "        \"plot_interval\": 1,\n",
    "        \n",
    "        # Logging\n",
    "        \"report_to\": [\"tensorboard\"],\n",
    "        \"logging_steps\": 5,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\"\n",
    "    }\n",
    "    \n",
    "    # Create trainer with all components\n",
    "    trainer = build_clm_trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        cfg=training_config,\n",
    "        output_dir=output_dir,\n",
    "        ard_prior_ratio=training_config[\"ard_prior_ratio\"],\n",
    "        enable_callbacks=training_config[\"enable_callbacks\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ ARD-LoRA trainer created successfully!\")\n",
    "    print(f\"üìä Training Configuration:\")\n",
    "    print(f\"   - Epochs: {training_config['train_epochs']}\")\n",
    "    print(f\"   - Batch size: {training_config['batch_size']}\")\n",
    "    print(f\"   - Gradient accumulation: {training_config['gradient_accumulation_steps']}\")\n",
    "    print(f\"   - Learning rate: {training_config['learning_rate']}\")\n",
    "    print(f\"   - KL beta: {training_config['beta']}\")\n",
    "    print(f\"   - Output directory: {output_dir}\")\n",
    "    \n",
    "    # Check callback registration\n",
    "    if hasattr(trainer, 'callback_handler') and trainer.callback_handler.callbacks:\n",
    "        callback_names = [type(cb).__name__ for cb in trainer.callback_handler.callbacks]\n",
    "        print(f\"   - Callbacks: {', '.join(callback_names)}\")\n",
    "    \n",
    "    return trainer, output_dir\n",
    "\n",
    "# Create trainer\n",
    "if 'train_dataset' in locals() and 'val_dataset' in locals():\n",
    "    trainer, output_directory = create_ard_trainer(model, tokenizer, train_dataset, val_dataset)\n",
    "    print(\"üéØ Ready for training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Datasets not loaded - please run Step 8 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27887113",
   "metadata": {},
   "source": [
    "## Step 10: Start ARD-LoRA Training with Uncertainty Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c70a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start comprehensive ARD-LoRA training\n",
    "def start_training():\n",
    "    print(\"üöÄ Starting ARD-LoRA training with uncertainty evaluation...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Initial evaluation\n",
    "        if val_dataset:\n",
    "            print(\"üîÑ Running initial evaluation...\")\n",
    "            initial_metrics = trainer.evaluate()\n",
    "            print(f\"üìä Initial eval loss: {initial_metrics.get('eval_loss', 'N/A'):.4f}\")\n",
    "        \n",
    "        # Start training with automatic callbacks\n",
    "        print(\"\\nüèãÔ∏è Training started...\")\n",
    "        print(\"Expected behavior each epoch:\")\n",
    "        print(\"  1. PriorEstimationCallback: Estimate ARD priors\")\n",
    "        print(\"  2. Training steps with KL regularization\")\n",
    "        print(\"  3. UncertaintyEvaluationCallback: Compute ACC, ECE, NLL\")\n",
    "        print(\"  4. LatentPlotCallback: Generate plots (epochs 2+)\")\n",
    "        print(\"  5. Model checkpoint saving\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        \n",
    "        # Train the model\n",
    "        training_results = trainer.train()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üéâ Training completed successfully!\")\n",
    "        print(f\"üìä Final training loss: {training_results.training_loss:.4f}\")\n",
    "        \n",
    "        # Display uncertainty evolution\n",
    "        if hasattr(trainer, 'uncertainty_results') and trainer.uncertainty_results:\n",
    "            print(f\"\\nüìà Uncertainty Evolution Across Epochs:\")\n",
    "            print(f\"{'Epoch':<8} {'Accuracy':<10} {'ECE':<10} {'NLL':<10}\")\n",
    "            print(\"-\" * 40)\n",
    "            for result in trainer.uncertainty_results:\n",
    "                epoch = result.get('epoch', '?')\n",
    "                acc = result.get('accuracy', 0)\n",
    "                ece = result.get('ece', 0)\n",
    "                nll = result.get('nll', 0)\n",
    "                print(f\"{epoch:<8} {acc:<10.4f} {ece:<10.4f} {nll:<10.4f}\")\n",
    "        \n",
    "        # Save final model\n",
    "        print(f\"\\nüíæ Saving final model...\")\n",
    "        trainer.save_model()\n",
    "        \n",
    "        # Save uncertainty results\n",
    "        if hasattr(trainer, 'uncertainty_results'):\n",
    "            results_file = f\"{output_directory}/uncertainty_evolution.json\"\n",
    "            import json\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(trainer.uncertainty_results, f, indent=2)\n",
    "            print(f\"üìä Uncertainty results saved to: {results_file}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ All results saved to: {output_directory}\")\n",
    "        print(f\"üìÅ Available in Google Drive: {output_directory.replace('/content/drive/MyDrive/', '')}\")\n",
    "        \n",
    "        return training_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        # Always clean up memory\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ Memory cleaned up\")\n",
    "\n",
    "# Start training\n",
    "if 'trainer' in locals():\n",
    "    training_results = start_training()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Trainer not created - please run previous steps first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe5b5b",
   "metadata": {},
   "source": [
    "## Step 11: Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize training results\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_results():\n",
    "    print(\"üìä Analyzing ARD-LoRA training results...\")\n",
    "    \n",
    "    # Load uncertainty results\n",
    "    results_file = f\"{output_directory}/uncertainty_evolution.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            uncertainty_results = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame for analysis\n",
    "        df = pd.DataFrame(uncertainty_results)\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('ARD-LoRA Training Results: Uncertainty Evolution', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Accuracy evolution\n",
    "        axes[0, 0].plot(df['epoch'], df['accuracy'], 'b-o', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_title('Accuracy (ACC) Evolution', fontsize=14)\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_ylim(0, 1)\n",
    "        \n",
    "        # Plot 2: ECE evolution\n",
    "        axes[0, 1].plot(df['epoch'], df['ece'], 'r-s', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_title('Expected Calibration Error (ECE)', fontsize=14)\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('ECE')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: NLL evolution\n",
    "        axes[1, 0].plot(df['epoch'], df['nll'], 'g-^', linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_title('Negative Log-Likelihood (NLL)', fontsize=14)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('NLL')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Combined normalized metrics\n",
    "        # Normalize metrics for comparison\n",
    "        acc_norm = df['accuracy'] / df['accuracy'].max()\n",
    "        ece_norm = 1 - (df['ece'] / df['ece'].max())  # Invert ECE (lower is better)\n",
    "        nll_norm = 1 - ((df['nll'] - df['nll'].min()) / (df['nll'].max() - df['nll'].min()))  # Invert NLL\n",
    "        \n",
    "        axes[1, 1].plot(df['epoch'], acc_norm, 'b-o', label='Accuracy (norm)', linewidth=2)\n",
    "        axes[1, 1].plot(df['epoch'], ece_norm, 'r-s', label='ECE (inv norm)', linewidth=2)\n",
    "        axes[1, 1].plot(df['epoch'], nll_norm, 'g-^', label='NLL (inv norm)', linewidth=2)\n",
    "        axes[1, 1].set_title('Normalized Metrics Comparison', fontsize=14)\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Normalized Score')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_ylim(0, 1.1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = f\"{output_directory}/uncertainty_evolution.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üìà Visualization saved to: {plot_path}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nüìä Training Summary:\")\n",
    "        print(f\"   Initial ‚Üí Final Accuracy: {df['accuracy'].iloc[0]:.4f} ‚Üí {df['accuracy'].iloc[-1]:.4f}\")\n",
    "        print(f\"   Initial ‚Üí Final ECE: {df['ece'].iloc[0]:.4f} ‚Üí {df['ece'].iloc[-1]:.4f}\")\n",
    "        print(f\"   Initial ‚Üí Final NLL: {df['nll'].iloc[0]:.4f} ‚Üí {df['nll'].iloc[-1]:.4f}\")\n",
    "        \n",
    "        improvement_acc = df['accuracy'].iloc[-1] - df['accuracy'].iloc[0]\n",
    "        improvement_ece = df['ece'].iloc[0] - df['ece'].iloc[-1]  # Lower is better\n",
    "        improvement_nll = df['nll'].iloc[0] - df['nll'].iloc[-1]  # Lower is better\n",
    "        \n",
    "        print(f\"\\nüìà Improvements:\")\n",
    "        print(f\"   Accuracy: {improvement_acc:+.4f} ({'‚úÖ Improved' if improvement_acc > 0 else '‚ùå Degraded'})\")\n",
    "        print(f\"   ECE: {improvement_ece:+.4f} ({'‚úÖ Improved' if improvement_ece > 0 else '‚ùå Degraded'})\")\n",
    "        print(f\"   NLL: {improvement_nll:+.4f} ({'‚úÖ Improved' if improvement_nll > 0 else '‚ùå Degraded'})\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Results file not found: {results_file}\")\n",
    "    \n",
    "    # Check for other artifacts\n",
    "    print(f\"\\nüìÅ Training Artifacts in {output_directory}:\")\n",
    "    if os.path.exists(output_directory):\n",
    "        for item in sorted(os.listdir(output_directory)):\n",
    "            item_path = os.path.join(output_directory, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                size_mb = os.path.getsize(item_path) / (1024**2)\n",
    "                print(f\"   üìÑ {item} ({size_mb:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"   üìÅ {item}/\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis complete! All files available in Google Drive.\")\n",
    "\n",
    "# Run analysis\n",
    "if 'output_directory' in locals():\n",
    "    analyze_results()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training results to analyze - please complete training first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e0761",
   "metadata": {},
   "source": [
    "## Step 12: Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e58a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download key results for local analysis\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "def download_results():\n",
    "    print(\"üì¶ Preparing results for download...\")\n",
    "    \n",
    "    if 'output_directory' not in locals():\n",
    "        print(\"‚ö†Ô∏è No training results available\")\n",
    "        return\n",
    "    \n",
    "    # Create a zip file with key results\n",
    "    zip_path = \"/content/ard_lora_results.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add key files\n",
    "        result_files = [\n",
    "            \"uncertainty_evolution.json\",\n",
    "            \"uncertainty_evolution.png\", \n",
    "            \"training_args.bin\",\n",
    "            \"trainer_state.json\"\n",
    "        ]\n",
    "        \n",
    "        for filename in result_files:\n",
    "            file_path = f\"{output_directory}/{filename}\"\n",
    "            if os.path.exists(file_path):\n",
    "                zipf.write(file_path, filename)\n",
    "                print(f\"   ‚úÖ Added {filename}\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è {filename} not found\")\n",
    "        \n",
    "        # Add model config if available\n",
    "        config_path = f\"{output_directory}/config.json\"\n",
    "        if os.path.exists(config_path):\n",
    "            zipf.write(config_path, \"config.json\")\n",
    "            print(f\"   ‚úÖ Added config.json\")\n",
    "    \n",
    "    print(f\"üì¶ Results packaged in: {zip_path}\")\n",
    "    \n",
    "    # Download the zip file\n",
    "    files.download(zip_path)\n",
    "    print(\"‚¨áÔ∏è Download started!\")\n",
    "\n",
    "# Uncomment to download results\n",
    "# download_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f064f7",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "**Congratulations!** You have successfully trained an ARD-LoRA model with comprehensive uncertainty evaluation.\n",
    "\n",
    "### **What Was Accomplished:**\n",
    "\n",
    "‚úÖ **ARD-LoRA Integration**: ProbLoRA layers injected into all LLaMA2-7B attention projections (q/k/v/o)\n",
    "\n",
    "‚úÖ **Bayesian-PEFT Compatibility**: Dataset loading consistent with https://github.com/Wang-ML-Lab/bayesian-peft\n",
    "\n",
    "‚úÖ **Uncertainty Evaluation**: ACC, ECE, and NLL computed after each epoch\n",
    "\n",
    "‚úÖ **ARD Prior Estimation**: Automatic relevance determination for layer importance\n",
    "\n",
    "‚úÖ **Google Drive Persistence**: All data and results cached for future use\n",
    "\n",
    "‚úÖ **Complete Callback System**: Prior estimation, latent plotting, and evaluation callbacks\n",
    "\n",
    "### **Key Results:**\n",
    "- **Model Checkpoints**: Saved after each epoch in Google Drive\n",
    "- **Uncertainty Evolution**: JSON file tracking ACC/ECE/NLL across epochs  \n",
    "- **Visualizations**: Plots showing uncertainty metric evolution\n",
    "- **ARD Analysis**: Layer-wise relevance determination results\n",
    "- **Tensorboard Logs**: Available for detailed training monitoring\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **Analyze Results**: Review uncertainty evolution and model calibration\n",
    "2. **Experiment with Hyperparameters**: Adjust KL beta, rank, or dataset\n",
    "3. **Extended Training**: Increase epochs for further improvement\n",
    "4. **Model Deployment**: Use trained model for downstream tasks\n",
    "\n",
    "All results are automatically saved to your Google Drive and will persist across Colab sessions! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
